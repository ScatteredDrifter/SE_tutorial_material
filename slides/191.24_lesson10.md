---
width: 900
height: 900
maxScale: 1.5
---

# SE-Tutorium 4 | Mittwoch 16 - 18 Uhr:
anchored to [[191.00_anchor]]

---

## Lernziele:

- Gründe / Motivation für Tests wiederholen
- Was macht gute Tests aus?
- Merkmale schlechter / suboptimaler Tests
- wie schreiben wir gute Tests?
- Unit Testing am Beispiel
- Whitebox / Blackbox testing
- Property Testing ( konzeptuell )


---
## | Orga | Aktuelle Abgaben
- **HW8** (weiterhin machbar!)-> bis zum **28.01**
- **HW10** verfügbar! --> Gruppen erstellen! **Abgabe am 28.01**
	- Erstellt die Gruppen und Teams! 
- Helpdesk jeden Freitag **16-18 Uhr** auf dem Sand! 
- Sorry, dass ich HW7 noch nicht korrigierte ( stress )

---

## Aufgabe | Vorbereitung Tutorium

>[!Task] Im REPO (**ex11-tut4**)
>1. Repository clonen!
>2. Projekt im Ordner **countClumps** öffnen
>3. `sbt compile` ausführen --> lädt Dependencies herunter ( brauch etwas)
>4. fertig 
>

---

## Automated Testing | Gründe

- (maintenance) Änderung einer Implementation --> Testen, ob Contract erhalten bleibt ( sonst Fehler!) 
- (confidence) Abdecken von Fehlern
- (development) einmal an spezielle Nits denken, danach automatisch 
- (robustness) Probleme nicht **wieder** einführen! 
- (documentation) Tests zeigen, wie Implementation funktionieren soll
- (reviewing) --> manuelles Testen von reviewer fällt weg!

---

## Automated Testing | Best Practices
- Tests können verschieden geschrieben werden 
- meist: **Qualität -> Quantität**! 
- zu viele Tests können Verständnis | Richtigkeit verwischen
- daher **Best-Practices** nach denen man sich orientieren kann!
	- finden sich auch im Online-Skript ( und Repo!)

--> Link zum REPO : https://github.com/se-tuebingen-exercises-ws23/ex11-tut4
--> Link zum Skript : https://se.cs.uni-tuebingen.de/teaching/ws23/se/skript/design-by-contract.html


---

## Automated Testing | Best Practices
(( Diese ganzen practices finden sich auch nochmal im REPO selbst :) ))
-> Wir sind sie im Tutorium durchgegangen und haben sie besprochen und die Gründe für diese Practices betrachtet und formuliert. 

1. Tests should be fast <!-- element class="fragment" -->
2. Tests should be cohesive, independent, and isolated <!-- element class="fragment" -->
3. Tests should have a reason to exist <!-- element class="fragment" -->
4. Tests should be repeatable and not flaky <!-- element class="fragment" -->
5. Tests should have strong assertions <!-- element class="fragment" -->
6. Tests should break in case the behaviour changes <!-- element class="fragment" -->
7. Tests should have a single and clear reason to fail <!-- element class="fragment" -->
8. Tests should be easy to write <!-- element class="fragment" -->
9. Tests should be easy to read <!-- element class="fragment" -->
10. Tests should be easy to change and evolve <!-- element class="fragment" -->

---

## Automated Testing | Probleme (Smells)

1. Excessive duplication
2. Unclear assertions
3. Bad handling of complex or external resources
4. Fixtures that are too general
5. Sensitive assertions

note: I may give some explanation on those

excessive duplication: if we have tests that double in their testing range or even their expression then why should we contain those? its added redundancy, time testing and it wont yield any information! 

unclear assertions: Maybe we've written a test that tests something, well but if it fails we may not know what failed or why it did ( reasoning about its assertion basically) --> what does it mean? 

bad handling of some complex / external resources: 
-> external resources **should not be tested** --> we are only using this implementation, why test someone elses library / code? --> we cannot ensure that it will work well 

fixtures that are too general --> will talk about fixtures in unit testing 
basically those are creating an _environment_ we will test / make our assumptions in --> if its too general ( testing nothing special), we may not be able to narrow down the error ( if we allow numbers from 0 to 100) but the error happened at 70 --> how should we know about that ? 

sensitive assertions 
-> well they should be sensitive **but only for the thing we want to test!** 
--> once again **not too general!**


---
## Automated Testing | Achtung!

- "Testing can only show **the presence** of bugs, **not their absence**" (Dijkstra)
- -> nur weil Tests keine Fehler zeigen, haben wir keinen perfekten Code
- --> Hohe Test-Coverage also keine gute Metrik für Fehler! 
	- ( wie jede Metrik, kann sie abused werden)

---

### Typen von Tests 

Wir möchten Tests in verschiedene Kategorien unterteilen, welche? 

( here I inserted a graphic from the lecture-slides where we split up testing into 4 large categories:
- Granularity
- Code Knowledge
- Style of Specificiation
- Software Attributes

--> Denote that we primarily focusing on **Granularity** and **Code Knowledge** here.
( more is likely reaching limits of this lectures scope and well another field to learn in )

For more information consider taking a look at the script once again :) 
https://se.cs.uni-tuebingen.de/teaching/ws23/se/skript/software-tests.html

---

## Unit Tests | kleine Teile Testen

- behandelt testen von **Granularen** Modulen
- dabei kann der Scope verschieden sein!
	- Library / Paket
	- große Klasse
	- ganzes Modul
	- einzelne Funktionen
	- ...
- **benötigt dafür:**
	- SUT - System under Test --> das zu testende  <!-- element class="fragment" -->
	- Fixture --> "Scope", den wir zum testen konstruieren ( environment to test in) <!-- element class="fragment" -->

---

## Unit Tests  | Aufbau

( inserted image from script ) ![image denoting structure of unit test](https://se.cs.uni-tuebingen.de/teaching/ws23/se/skript/software-tests/phases.png)

wir haben diverse Punkte, die wir durchlaufen müssen, wenn wir Unit-Tests nutzen möchten ||

1 Setup --> Zustand herstellen, den wir zum testen brauch. --> Fixture aufbauen
- kann etwa spezielles Objekt sein, was wir passend initialisieren müssen

2 Interact --> mit dem SUT "interagieren" --> Methoden, Funktionen etc

3 Assert --> Die **Ergebnisse** von Interact gegen die erwartetenden Werte testen! ( Also etwa $add(2,2) == 4$)
- hier können bei falschen Asserts schon Fehler auftreten und uns alamieren! 

4 Teardown --> die "Fixture" abbauen | Environment zum testen entfernen

---

## Testing ist auch SE |
- Tests zum Validieren der Funktionalität des getesteten Codes
- ! Wenn Implementation geändert wird, failen Tests auch ! 
- -> am besten Tests einfach schreiben können ( sonst zu viele Hürden / unlukrativ )
- Drei Methoden wurden dafür kennen gelernt _Welche?_

---

## Testing ist auch SE |
- **Creation Methods** --> Helfen uns schnell "Setup" abarbeiten zu können ( bauen Datenstrukturen / Objekte --> Fixture)
- **Encapsulation Methods** --> Vereinfachen der Interaktion mit SUT ( i.e. komplexes Setup zum Aufrufen / Interaktion mit Sut notwendig)
- **Assertion Methods** --> Gemäß gegebener Vergleichsparameter testen ( Vergleiche erwarteten Output mit tatsächlichem)

---
## Weitere "Granularity-Tests"
- **Integration Tests** -> testet Interaktion von diversen Modulen 
	- Primär um Fehler zu finden, die beim Zusammenspiel auftreten
- "**End-To-End Tests**" -> Ebene höher: Testen gesamte Anwendung ( statt Module) --> eher auf Anwendenden-Scope 

---
### "Code Knowledge" | Specification-based Testing
- "Black-Box Testing"
- Wir möchten mit unseren Tests gegen eine **Spezifikation** testen
	- Contracts o.ä! 
- Wissen über Implementation sollte nicht **benötigt werden** 

### Specification-based Testing | Ablauf
1. Specification, die gegeben ist, verstehen (i.e Contract | genannte Pre/Post-Conditions)
2. Programm anschauen ( wenn möglich!) --> manchmal bekommt man nur die Specifications!
3. Partitionen erkennen --> herausfinden, welche **Eingaben**, welche **Ausgaben** fordern

---

### Specification-based Testing | Ablauf
```
def isPositive(n:Int):Boolean
```
- --> Welche Äquivalenzklassen können wir setzen? 
- Randfälle betrachten? 

note: 
Wir wissen anschließend also, dass hier alles >0 true geben muss ( nach **Spezifikation!**)
und alles < 0 nicht positiv
was ist mir $=0$ ? --> positiv oder negativ?  --> RANDFALL! den wir jetzt ausmachen können ( jenachdem, wie es etwa der Contract der Spezifikation angibt :) )

---

### Specification-based Testing | Ablauf

4. Mögliche Randfälle betrachten 
5. Tests schreiben, die unsere **Partitionen** und **Randfälle** abdecken wird
6. Testen! 

Darstellung des Ablaufs findet sich auch nochmal hier: https://se.cs.uni-tuebingen.de/teaching/ws23/se/skript/software-tests/black-box.html

---

### Unit Tests  | Black-Box Tests | Üben ! 

>[!Task] IM Repo 
>1. Task 2 anschauen und durcharbeiten 
>2. Bearbeite bis zu Subtask **10!**
--> Dabei sollte man hier vor Allem versuchen nicht die Implementation des "countClumps" anzuschauen,
weil wir ja auch anhand der Spezifikation Test schreiben und somit validieren können, ob sie ihren Contract einhält.


---

## Structure testing | White-Box Testing
- mit **Black-Box** Spezifikation testen ( ohne Wissen über Implementation)
- mit **White-Box** --> Implementation testen
- wir möchten **alle möglichen Zustände** abdecken! ( testen im besten Fall)
- **drauf achten**, dass alle möglichen (Paths genommen werden)
- weitere Infos ( und Beispiele ) https://se.cs.uni-tuebingen.de/teaching/ws23/se/skript/software-tests/white-box.html

---
### Unit Tests | White-Box Tests | Üben ! 

>[!Task] IM Repo 
>1. Subtask **11** für Aufgabe 2 anschauen und bearbeiten


---

## Metriken | 
- Metriken ( jede ) kann man irgendwie betrügen 
- Daher ist **Coverage** nicht immer sinnig 
- ( Abhängig von Qualität der Tests ) 

---

## Metriken | 
>[!Task] IM REPO 
>1. Aufgabe 3 anschauen und selbständig bearbeiten
>2. nach 10 min Bearbeitungszeit tragen wir zusammen 

---

## Mutation Testing | Prinzip
- Mutation Testing zählt zu **White-Box** Testing, denn
- Implementation verändert sich! --> SUT wird ( intern ) verändert
- dadurch können neue Zustände auftreten --> **Tests** sollten diese finden und lösen
- **Motivation**: Mit "mutierten Implementationen" können wir 
	- **schwache Tests** finden
	- **neue Tests** erstellen
- Mutation Testing --> kann die vorherige Metrik invalidieren!

---

## Mutation Testing | Üben 

>[!Task] IM REPO 
>1. Aufgabe 4 anschauen und selbständig bearbeiten
>2. nach 20-25 min Bearbeitungszeit tragen wir zusammen 

---

## Meta Tests | 

- prinzipiell ähnlich / gleich zu Mutation Testing, aber **manuell**
- also selbst gesetzte Veränderungen in der Implementation
	- werden auch in der HW genutzt ( aber nicht einsehbar!)

---

## Meta Tests | Übung

>[!Task] IM REPO 
>1. Aufgabe 5 anschauen und selbständig bearbeiten
>2. nach 10-15 min Bearbeitungszeit tragen wir zusammen 

note: 
what we can draw from this: 
-> tests might have to **change** whenever we are changing the implementation itself
--> updating it and all might be necessary sometimes!
---> Hence it should be easy to update / write tests!

---

## Property-based Testing
- zuvor bestimmte Äquivalenzklassen (Unit-Testing) geben Beziehungen zwischen Partitionen (Ein/Ausgabe) an
- wir nutzen **keine Beispielwerte** ( selbst gewählten) --> **Generieren von Werten** zum testen
- Dafür brauch es:
	- **Generator** --> Erstellt Werte zum Testen für eine bestimmte Partition ( nur negative Zahlen )
	- **Property** --> Assert, der für die Werte aus dem Generator geprüft wird 
- --> Generierung kann alle Fälle selbst abdecken, **wir müssen sie nicht schreiben!**

---

## Property-based Testing  | Üben

>[!Task] IM REPO 
>1. Aufgabe 5 anschauen und selbständig bearbeiten
>2. nach 20-25 min Bearbeitungszeit tragen wir zusammen 

